"""
Claude API Client - Interface pour communiquer avec Claude 4.5
"""
import os
import logging
import anthropic
from typing import Dict, List, Optional, Any
from pathlib import Path
import sys
from dotenv import load_dotenv

# Charger les variables d'environnement depuis .env
load_dotenv()

# Ajouter le dossier src au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.config_loader import config

class ClaudeClient:
    """Client pour interagir avec l'API Claude d'Anthropic."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # V√©rifier si on force le mode mock
        force_mock = os.getenv('CLAUDE_MOCK_MODE', 'false').lower() == 'true'
        
        # R√©cup√©rer la cl√© API depuis les variables d'environnement
        api_key_env = config.get('claude.api_key_env', 'ANTHROPIC_API_KEY')
        self.api_key = os.getenv(api_key_env)
        
        if not self.api_key or force_mock:
            self.logger.warning(f"Mode MOCK activ√© (force_mock={force_mock}, has_key={bool(self.api_key)})")
            self.logger.info("Les r√©ponses seront simul√©es")
            self.mock_mode = True
            self.client = None
        else:
            self.mock_mode = False
            try:
                self.client = anthropic.Anthropic(api_key=self.api_key)
                self.logger.info("Client Claude initialis√© avec API r√©elle")
            except Exception as e:
                self.logger.error(f"Erreur initialisation client: {e}")
                self.mock_mode = True
                self.client = None
        
        # Param√®tres par d√©faut
        self.model = config.get('claude.model', 'claude-sonnet-4-20250514')
        self.max_tokens = config.get('claude.max_tokens', 4096)
        self.temperature = config.get('claude.temperature', 0.7)
        
        self.logger.info(f"ClaudeClient initialis√© (Mock: {self.mock_mode})")
    
    def send_message(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Envoie un message √† Claude et retourne la r√©ponse.
        
        Args:
            prompt: Le prompt √† envoyer
            system_prompt: Instructions syst√®me optionnelles
            max_tokens: Nombre maximum de tokens (d√©faut: config)
            temperature: Temp√©rature de g√©n√©ration (d√©faut: config)
            conversation_history: Historique de conversation optionnel
            
        Returns:
            Dictionnaire contenant la r√©ponse et les m√©tadonn√©es
        """
        # Mode mock pour les tests sans API key
        if self.mock_mode:
            return self._mock_response(prompt)
        
        # Utiliser les valeurs par d√©faut si non sp√©cifi√©es
        max_tokens = max_tokens or self.max_tokens
        temperature = temperature or self.temperature
        
        try:
            # Construire les messages
            messages = []
            
            # Ajouter l'historique si fourni
            if conversation_history:
                messages.extend(conversation_history)
            
            # Ajouter le message actuel
            messages.append({
                "role": "user",
                "content": prompt
            })
            
            # Appeler l'API Claude
            self.logger.info(f"Envoi message √† Claude (tokens: {max_tokens})")
            
            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt if system_prompt else "",
                messages=messages
            )
            
            # Extraire la r√©ponse
            response_text = response.content[0].text
            
            result = {
                "success": True,
                "response": response_text,
                "model": response.model,
                "tokens_used": {
                    "input": response.usage.input_tokens,
                    "output": response.usage.output_tokens,
                    "total": response.usage.input_tokens + response.usage.output_tokens
                },
                "stop_reason": response.stop_reason
            }
            
            self.logger.info(f"R√©ponse re√ßue ({result['tokens_used']['total']} tokens)")
            return result
            
        except anthropic.APIError as e:
            if "credit balance" in str(e).lower():
                self.logger.warning("Cr√©dits API insuffisants, basculement en mode MOCK")
                self.mock_mode = True
                return self._mock_response(prompt)
            else:
                self.logger.error(f"Erreur API Claude: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "response": None
                }
        except Exception as e:
            self.logger.error(f"Erreur lors de l'appel √† Claude: {e}")
            return {
                "success": False,
                "error": str(e),
                "response": None
            }
    
    def _mock_response(self, prompt: str) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse mock√©e pour les tests.
        
        Args:
            prompt: Le prompt envoy√©
            
        Returns:
            R√©ponse simul√©e
        """
        self.logger.info("G√©n√©ration d'une r√©ponse mock√©e")
        
        # Analyser le prompt pour donner une r√©ponse contextuelle
        prompt_lower = prompt.lower()
        
        if "temp√©rature" in prompt_lower or "temperature" in prompt_lower:
            mock_text = """
Analyse de la situation thermique :

**Diagnostic :**
La temp√©rature de 95¬∞C d√©tect√©e est au seuil critique. Cela indique :
- Possible surchauffe du moteur
- Risque de d√©faillance imminente
- Besoin d'intervention urgente

**Recommandations imm√©diates :**
1. üõë Arr√™t imm√©diat de l'√©quipement
2. üåÄ Activation du syst√®me de refroidissement
3. üìû Alerte √† l'√©quipe de maintenance
4. üîç Inspection visuelle requise

**S√©v√©rit√© :** CRITIQUE
**Confiance :** 92%
"""
        elif "vibration" in prompt_lower:
            mock_text = """
Analyse des vibrations :

**Diagnostic :**
Vibrations anormales d√©tect√©es au-del√† des seuils normaux.

**Causes possibles :**
- D√©salignement des composants
- Usure des roulements
- Fixations desserr√©es

**Actions recommand√©es :**
1. üîç Inspection visuelle imm√©diate
2. üìê Mesure pr√©cise des vibrations
3. üîß V√©rification alignement et fixations

**S√©v√©rit√© :** √âLEV√âE
**Confiance :** 89%
"""
        elif "pression" in prompt_lower or "pressure" in prompt_lower:
            mock_text = """
Analyse de pression :

**ALERTE CRITIQUE :**
Surpression syst√®me d√©tect√©e !

**Risques :**
- Risque d'explosion
- Risque de rupture
- Danger imm√©diat

**ACTIONS URGENTES :**
1. üö® √âVACUATION IMM√âDIATE de la zone
2. üõë Fermeture vannes de s√©curit√©
3. üìû Appel services d'urgence

**S√©v√©rit√© :** CRITIQUE
**Confiance :** 95%
"""
        else:
            mock_text = f"""
Analyse de la situation :

J'ai re√ßu votre demande concernant : "{prompt[:100]}..."

**√âvaluation :**
Situation n√©cessitant une analyse approfondie.

**Recommandations :**
1. Collecte de donn√©es suppl√©mentaires
2. Surveillance accrue
3. Consultation avec l'√©quipe SST

**S√©v√©rit√© :** MOD√âR√âE
**Confiance :** 85%
"""
        
        return {
            "success": True,
            "response": mock_text.strip(),
            "model": f"{self.model} (MOCK)",
            "tokens_used": {
                "input": len(prompt.split()),
                "output": len(mock_text.split()),
                "total": len(prompt.split()) + len(mock_text.split())
            },
            "stop_reason": "end_turn",
            "mock": True
        }
    
    def analyze_sst_situation(self, situation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyse une situation SST avec Claude.
        
        Args:
            situation: Dictionnaire d√©crivant la situation
            
        Returns:
            Analyse et recommandations
        """
        # Construire le prompt
        prompt = f"""
Vous √™tes un expert en sant√© et s√©curit√© au travail (SST).

Analysez la situation suivante et fournissez :
1. Un diagnostic clair
2. Le niveau de s√©v√©rit√© (CRITIQUE, √âLEV√âE, MOD√âR√âE, FAIBLE)
3. Les risques identifi√©s
4. Les actions recommand√©es (prioris√©es)
5. Votre niveau de confiance (%)

**Situation :**
{situation.get('description', 'Non sp√©cifi√©e')}

**Donn√©es techniques :**
"""
        
        # Ajouter les param√®tres techniques
        params = situation.get('parameters', {})
        for key, value in params.items():
            prompt += f"- {key}: {value}\n"
        
        prompt += """

Fournissez une analyse structur√©e et des recommandations actionnables.
"""
        
        system_prompt = """Vous √™tes un assistant IA sp√©cialis√© en sant√© et s√©curit√© au travail.
Vous analysez les situations avec rigueur, identifiez les risques et proposez des actions concr√®tes.
Vous √™tes direct, pr√©cis et orient√© vers la pr√©vention."""
        
        return self.send_message(
            prompt=prompt,
            system_prompt=system_prompt
        )


# Exemple d'utilisation
if __name__ == "__main__":
    # Configuration du logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("="*60)
    print("Test du Claude API Client")
    print("="*60)
    
    # Cr√©er le client
    client = ClaudeClient()
    
    if client.mock_mode:
        print("\n‚ö†Ô∏è  Mode MOCK activ√©")
        print("Les r√©ponses seront simul√©es pour les tests.\n")
    else:
        print("\n‚úÖ Client Claude initialis√© avec API r√©elle\n")
    
    # Test 1: Analyse temp√©rature
    print("\n" + "="*60)
    print("Test 1: Analyse de temp√©rature critique")
    print("="*60)
    
    situation1 = {
        "description": "Temp√©rature moteur anormalement √©lev√©e d√©tect√©e",
        "parameters": {
            "temperature": "95¬∞C",
            "machine_id": "M-47",
            "location": "Ligne production A"
        }
    }
    
    result1 = client.analyze_sst_situation(situation1)
    
    if result1['success']:
        print(f"\n‚úÖ Analyse r√©ussie")
        print(f"üìä Tokens utilis√©s: {result1['tokens_used']['total']}")
        print(f"\nüìã R√©ponse de Claude:\n")
        print(result1['response'])
    else:
        print(f"\n‚ùå Erreur: {result1.get('error', 'Unknown')}")
    
    # Test 2: Analyse vibrations
    print("\n" + "="*60)
    print("Test 2: Analyse de vibrations anormales")
    print("="*60)
    
    situation2 = {
        "description": "Vibrations anormales d√©tect√©es sur presse hydraulique",
        "parameters": {
            "vibration": "8.2 mm/s",
            "machine_id": "PH-12",
            "normal_range": "< 3 mm/s"
        }
    }
    
    result2 = client.analyze_sst_situation(situation2)
    
    if result2['success']:
        print(f"\n‚úÖ Analyse r√©ussie")
        print(f"üìä Tokens utilis√©s: {result2['tokens_used']['total']}")
        print(f"\nüìã R√©ponse de Claude:\n")
        print(result2['response'])
    else:
        print(f"\n‚ùå Erreur: {result2.get('error', 'Unknown')}")
    
    print("\n" + "="*60)
    print("Tests termin√©s !")
    print("="*60)
